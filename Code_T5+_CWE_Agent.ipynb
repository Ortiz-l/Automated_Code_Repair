{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c820ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6585ecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hugging face personal access token from file\n",
    "with open(\"hf_pat.txt\", \"r\") as f:\n",
    "    token = f.read().strip()\n",
    "\n",
    "with open(\"oai_pat.txt\", \"r\") as f:\n",
    "    openai_key = f.read().strip()\n",
    "\n",
    "# Set it for HuggingFaceHub\n",
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = token\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fca65ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cwe_id_only(raw_line: str) -> str:\n",
    "    \"\"\"Extracts numeric CWE ID from a line like 'CWE-502: Deserialization of Untrusted Data'\"\"\"\n",
    "    match = re.search(r\"CWE-(\\d+)\", raw_line)\n",
    "    return match.group(1) if match else raw_line.strip()\n",
    "\n",
    "def run_router(cwe_id: str) -> tuple[str, str]:\n",
    "    \"\"\"Calls Layer 1 agents via router to generate context and prompt\"\"\"\n",
    "    from router import router  \n",
    "    from llm_tools import zephyr_tool \n",
    "    result = router(cwe_id, zephyr_tool)\n",
    "    return result.get(\"cwe_context\", \"\"), result.get(\"agent_prompt\", \"\")\n",
    "\n",
    "def extract_cwe_context(context_line: str) -> str:\n",
    "    \"\"\"Extracts additional context from the CWE context input into the model\"\"\"\n",
    "    parts = context_line.strip().split(\"||\")\n",
    "    return parts[1].strip() if len(parts) > 1 else \"\"\n",
    "\n",
    "def extract_agent_prompt(context_line: str) -> str:\n",
    "    \"\"\"Extracts the prompt for the agent from router input\"\"\"\n",
    "    parts = context_line.strip().split(\"||\")\n",
    "    return parts[2].strip() if len(parts) > 2 else \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e48035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_folders(root_dir):\n",
    "    \"\"\"Loads input data from the local directory as defined.\"\"\"\n",
    "    fine_tuning_data = []\n",
    "\n",
    "    for repo_folder in os.listdir(root_dir):\n",
    "        repo_path = os.path.join(root_dir, repo_folder)\n",
    "        file_groups = {}\n",
    "\n",
    "        for filename in os.listdir(repo_path):\n",
    "            match = re.match(r\"(.+)_(\\d+)\\.txt$\", filename)\n",
    "            if match:\n",
    "                prefix, number = match.groups()\n",
    "                number = int(number)\n",
    "                if number not in file_groups:\n",
    "                    file_groups[number] = {}\n",
    "                if \"source\" in prefix:\n",
    "                    file_groups[number][\"source\"] = os.path.join(repo_path, filename)\n",
    "                elif \"target\" in prefix:\n",
    "                    file_groups[number][\"target\"] = os.path.join(repo_path, filename)\n",
    "                elif \"context\" in prefix:\n",
    "                    file_groups[number][\"context\"] = os.path.join(repo_path, filename)\n",
    "\n",
    "        for number, files in sorted(file_groups.items()):\n",
    "            source_file = files.get(\"source\")\n",
    "            target_file = files.get(\"target\")\n",
    "            context_file = files.get(\"context\")\n",
    "\n",
    "            if source_file and target_file and context_file:\n",
    "                with open(source_file, \"r\", encoding=\"utf-8\") as src, \\\n",
    "                     open(target_file, \"r\", encoding=\"utf-8\") as tgt, \\\n",
    "                     open(context_file, \"r\", encoding=\"utf-8\") as ctx:\n",
    "\n",
    "                    sources = src.readlines()\n",
    "                    targets = tgt.readlines()\n",
    "                    contexts = ctx.readlines()\n",
    "\n",
    "                    for s, t, c in zip(sources, targets, contexts):\n",
    "                        cwe_id = extract_cwe_id_only(c)\n",
    "\n",
    "                        # Calls updated router function that invokes LangChain agents\n",
    "                        try:\n",
    "                            cwe_context, agent_prompt = run_router(cwe_id)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Failed to route CWE-{cwe_id}: {e}\")\n",
    "                            continue\n",
    "\n",
    "                        if cwe_id == \"78\":\n",
    "                            combined_input = f\"CWE-{cwe_id} Fix Request:\\n{s.strip()}\\nIssue: Command Injection (CWE-78)\\nFix:\"\n",
    "                        else:\n",
    "                            combined_input = f\"CWE-{cwe_id}\\n{cwe_context}\\n{agent_prompt}\\nCode: {s.strip()}\"\n",
    "\n",
    "\n",
    "                        fine_tuning_data.append({\n",
    "                            \"cwe_id\": cwe_id,\n",
    "                            \"cwe_context\": cwe_context,\n",
    "                            \"agent_prompt\": agent_prompt,\n",
    "                            \"code\": s.strip(),\n",
    "                            \"source\": combined_input,\n",
    "                            \"target\": t.strip()\n",
    "                        })\n",
    "\n",
    "    return fine_tuning_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4caf3713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "fine_tuning_data = load_data_from_folders(\"E:/Data Collection/Gold_Standard_for_tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7348e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4090\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ef5149ebc2469fba879a41c7438b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/72 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba093aea0bf24d1284b8ff583aa54a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laure\\AppData\\Local\\Temp\\ipykernel_26032\\2434105495.py:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90/90 36:46, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Codebleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.112200</td>\n",
       "      <td>1.900841</td>\n",
       "      <td>47.572816</td>\n",
       "      <td>54.221165</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>24.408615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.388200</td>\n",
       "      <td>1.748871</td>\n",
       "      <td>50.996993</td>\n",
       "      <td>62.502491</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>29.593473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.840400</td>\n",
       "      <td>1.764852</td>\n",
       "      <td>55.462921</td>\n",
       "      <td>64.967200</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>32.260414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.745600</td>\n",
       "      <td>1.758283</td>\n",
       "      <td>57.317605</td>\n",
       "      <td>67.387335</td>\n",
       "      <td>22.222222</td>\n",
       "      <td>38.038599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.386300</td>\n",
       "      <td>1.921272</td>\n",
       "      <td>57.802095</td>\n",
       "      <td>66.809225</td>\n",
       "      <td>22.222222</td>\n",
       "      <td>33.813879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.315500</td>\n",
       "      <td>1.878762</td>\n",
       "      <td>57.566852</td>\n",
       "      <td>66.300161</td>\n",
       "      <td>27.777778</td>\n",
       "      <td>40.201706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.185800</td>\n",
       "      <td>1.887004</td>\n",
       "      <td>55.487267</td>\n",
       "      <td>64.986625</td>\n",
       "      <td>22.222222</td>\n",
       "      <td>29.125037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.143800</td>\n",
       "      <td>1.927315</td>\n",
       "      <td>49.598683</td>\n",
       "      <td>64.675491</td>\n",
       "      <td>22.222222</td>\n",
       "      <td>29.090819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>1.941606</td>\n",
       "      <td>52.174322</td>\n",
       "      <td>67.305476</td>\n",
       "      <td>27.777778</td>\n",
       "      <td>28.929189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 01:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.941605806350708, 'eval_bleu': 52.1743223895892, 'eval_rougeL': 67.30547606656998, 'eval_accuracy': 27.77777777777778, 'eval_codebleu': 28.929188825401454, 'eval_runtime': 122.8682, 'eval_samples_per_second': 0.146, 'eval_steps_per_second': 0.041, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from codebleu import calc_codebleu\n",
    "import os\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "import torch\n",
    "# Identifies and prints the name of available GPUs\n",
    "print(torch.cuda.is_available())          \n",
    "print(torch.cuda.get_device_name(0))      \n",
    "\n",
    "model_name = \"Salesforce/codet5p-770m\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Converts list of data to a Hugging Face Dataset\n",
    "dataset = Dataset.from_list(fine_tuning_data)\n",
    "\n",
    "# Splits data into 80-20 train-test split\n",
    "split_dataset = dataset.train_test_split(test_size=0.2)\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the source inputs\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"source\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    # Tokenizes the targets (labels)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"target\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "    # Replaces padding token ids in labels with -100 so they are ignored the loss function\n",
    "    labels_ids = labels[\"input_ids\"]\n",
    "    labels_ids = [\n",
    "        [(token if token != tokenizer.pad_token_id else -100) for token in label]\n",
    "        for label in labels_ids\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels_ids\n",
    "    return model_inputs\n",
    "\n",
    "# Preprocesses test and train datasets\n",
    "train_dataset = split_dataset[\"train\"].map(preprocess_function, batched=True)\n",
    "test_dataset = split_dataset[\"test\"].map(preprocess_function, batched=True)\n",
    "'''\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"source\"], padding=\"max_length\", truncation=True, max_length=256\n",
    "    )\n",
    "    #128 or 256\n",
    "    \n",
    "    labels = tokenizer(\n",
    "        examples[\"target\"], padding=\"max_length\", truncation=True, max_length=256\n",
    "    )\n",
    "\n",
    "   \n",
    "    # Ensure `labels` is a **flat list** of token IDs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    print(\"Fixed Labels: \", model_inputs[\"labels\"][:2])  # Debugging print\n",
    "    return model_inputs\n",
    "'''\n",
    "\n",
    "\n",
    "import evaluate \n",
    "from sacrebleu import corpus_bleu\n",
    "import numpy as np\n",
    "from pygments.lexers import guess_lexer\n",
    "from pygments.util import ClassNotFound\n",
    "\n",
    "\n",
    "def detect_language(code_snippet):\n",
    "    \"\"\"Detects the programming language and maps it to a valid CodeBLEU language.\"\"\"\n",
    "    from pygments.lexers import guess_lexer\n",
    "    from pygments.util import ClassNotFound\n",
    "\n",
    "    AVAILABLE_LANGS = {'java', 'javascript', 'c_sharp', 'php', 'c', 'cpp', 'python', 'go', 'ruby', 'rust'}\n",
    "    \n",
    "    try:\n",
    "        lexer = guess_lexer(code_snippet)\n",
    "        lang = lexer.name.lower()\n",
    "    except ClassNotFound:\n",
    "        lang = \"unknown\"\n",
    "\n",
    "    # Maps detected language to CodeBLEU supported languages\n",
    "    lang_map = {\n",
    "        \"c++\": \"cpp\",\n",
    "        \"c#\": \"c_sharp\",\n",
    "        \"javascript\": \"javascript\",\n",
    "        \"java\": \"java\",\n",
    "        \"python\": \"python\",\n",
    "        \"php\": \"php\",\n",
    "        \"go\": \"go\",\n",
    "        \"ruby\": \"ruby\",\n",
    "        \"rust\": \"rust\",\n",
    "        \"c\": \"c\"\n",
    "    }\n",
    "    \n",
    "    mapped_lang = lang_map.get(lang, \"python\")  \n",
    "    return mapped_lang if mapped_lang in AVAILABLE_LANGS else \"python\"\n",
    "\n",
    "# Loads metrics\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    # Ensures labels are a clean list of lists containing only ints\n",
    "    labels = [[token if token != -100 else tokenizer.pad_token_id for token in label] for label in labels]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    lang = detect_language(decoded_preds[0]) or \"python\"\n",
    "\n",
    "    bleu_score = corpus_bleu(decoded_preds, [decoded_labels]).score\n",
    "    accuracy = sum(p == l for p, l in zip(decoded_preds, decoded_labels)) / len(decoded_preds)\n",
    "    rouge_scores = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    rouge_l = rouge_scores[\"rougeL\"]\n",
    "\n",
    "    codebleu_result = calc_codebleu(decoded_labels, decoded_preds, lang)\n",
    "    codebleu_score = float(codebleu_result.get(\"codebleu\", 0.0)) if isinstance(codebleu_result, dict) else float(codebleu_result)\n",
    "\n",
    "    return {\n",
    "        \"bleu\": bleu_score,\n",
    "        \"rougeL\": rouge_l * 100,\n",
    "        \"accuracy\": accuracy * 100,\n",
    "        \"codebleu\": codebleu_score * 100\n",
    "    }\n",
    "\n",
    "\n",
    "from transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",   \n",
    "    save_strategy=\"steps\",        \n",
    "    load_best_model_at_end=True,          \n",
    "    metric_for_best_model=\"eval_codebleu\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=5,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759c86f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Loads local fine-tuned CodeT5+ model and tokenizer\n",
    "model_path = \"C:/Users/laure/OneDrive/Documents/Benchmark Experiments/.Salesforce/codet5-base-finetuned/checkpoint-180\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "def run_codet5_repair(fine_tuning_data, max_input_tokens=512, max_output_tokens=256):\n",
    "    \"\"\"Finetunes the codet5+ repair portion in the agentic workflow.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for item in fine_tuning_data:\n",
    "        input_text = item[\"source\"]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_input_tokens\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=max_output_tokens,\n",
    "                do_sample=False\n",
    "            )\n",
    "\n",
    "        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        results.append({\n",
    "            \"cwe_id\": item[\"cwe_id\"],\n",
    "            \"original_code\": item[\"code\"],\n",
    "            \"agent_prompt\": item[\"agent_prompt\"],\n",
    "            \"cwe_context\": item[\"cwe_context\"],\n",
    "            \"generated_fix\": prediction,\n",
    "            \"target_fix\": item[\"target\"]\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d5ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWE-502 | Prediction: model = buffer;\n",
      "CWE-939 | Prediction: if fmt == \"url\":\n",
      "CWE-78 | Prediction: output0 = subprocess.run(cmd0, shell=False, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
      "Issue: Command Injection (CWE-78)\n",
      "Fix:\n",
      "CWE-78 | Prediction: output1 = subprocess.run(cmd1, shell=False, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
      "Issue: Command Injection (CWE-78)\n",
      "Fix:\n",
      "CWE-78 | Prediction: if not (debug and os.path.exists('vdos.out')):\n",
      "CWE-611 | Prediction: import defusedxml.etree.ElementTree as ET\n",
      "CWE-269 | Prediction: CMD streamlit run main.py --server.port $PORT -u **INSERT USER NUMBER HERE**\n",
      "CWE-89 | Prediction: values_x = ','.join(['%s'] * len(line[1]))\n",
      "CWE-676 | Prediction: dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=batch_size, collate_fn=collate, pin_memory=True)\n",
      "CWE-502 | Prediction: void function() {\n",
      "    strcpy(buffer, \"attacker input\");\n",
      "    void* buffer_ptr = buffer;\n",
      "    // continue using buffer here\n",
      "Code: return\n",
      "CWE-939 | Prediction: for category in json.loads(urlopen(url).read().decode('ascii'))\n",
      "CWE-326 | Prediction: self.sock = ssl.wrap_socket(sock, self.key_file, self.cert_file,\n",
      "CWE-78 | Prediction: subprocess.run(cmd,shell=False,check=True,timeout=timeout)\n",
      "CWE-78 | Prediction: p = Popen(cmd,shell=False,stdout=PIPE,stderr=PIPE)\n",
      "CWE-78 | Prediction: errorcode = subprocess.call(self.command, shell=False)\n",
      "CWE-78 | Prediction: result = subprocess.check_output('{} {} {}'.format(d3cmd, poscarfn, self.cmd_option_string), shell=False)\n",
      "CWE-78 | Prediction: process = subprocess.Popen(self.mopaccmd, shell=False, cwd=wdir, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
      "CWE-78 | Prediction: p = subprocess.Popen([zindobin], stdin=dinpf, stdout=doutf, stderr=derrf, shell=False, env=env)\n",
      "CWE-676 | Prediction: dataloader = DataLoader(dataset, batch_size=64, pin_memory=True)\n",
      "CWE-327 | Prediction: cnonce = (hashlib.sha3_256(s).hexdigest()[:16])\n",
      "CWE-502 | Prediction: void function() {\n",
      "    strcpy(buffer, \"attacker input\");\n",
      "    void* buffer_ptr = buffer;\n",
      "    // continue using buffer here\n",
      "Code: return\n",
      "CWE-611 | Prediction: import defusedxml.etree.ElementTree as ET\n",
      "CWE-502 | Prediction: cpp\n",
      "char buffer[100];\n",
      "\n",
      "void function() {\n",
      "    strcpy(buffer, \"attacker input\");\n",
      "    void* buffer_ptr = buffer;\n",
      "    // continue using buffer here\n",
      "Code: df\n",
      "\n",
      "CWE-915 | Prediction: ```c++\n",
      "void *my_malloc(size_t size) {\n",
      "    void *ptr = malloc(size);\n",
      "    return ptr;\n",
      "}\n",
      "```\n",
      "CWE-79 | Prediction: var createTableTh = function(j) {\n",
      "CWE-79 | Prediction: td.textContext = String(dataset.e(i, j));\n",
      "CWE-416 | Prediction: rslt = \"\"\"\\\n",
      "CWE-416, also known as Uncontrolled Resource Consumption or Resource Leak, refers to a situation where an application does not release resources it has allocated, leading to a waste of system resources or denial of service attacks, or the execution of arbitrary code or disclosure of sensitive information.\n",
      "CWE-415 | Prediction: if (mystr) {\n",
      "CWE-415 | Prediction: if (mystr) {\n",
      "CWE-415 | Prediction: if (str) {\n",
      "CWE-269 | Prediction: # Copyright 2020-2021 Efabless Corporation\n",
      "CWE-78 | Prediction: state = subprocess.run(\n",
      "Issue: Command Injection (CWE-78)\n",
      "CWE-89 | Prediction: from sqlalchemy import create_engine, MetaData, Table, text\n",
      "CWE-611 | Prediction: import defusedxml.etree.ElementTree as ET\n",
      "CWE-798 | Prediction: -----BEGIN RSA PRIVATE KEY-----\n",
      "CWE-78 | Prediction: process = subprocess.Popen([cmd], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=False)\n",
      "CWE-134 | Prediction: snprintf(\"Could not write to file: %s\\n\", argv[2]);\n",
      "CWE-134 | Prediction: snprintf(\"Could not open keys file: %s\\n\", argv[5]);\n",
      "CWE-134 | Prediction: snprintf(\"Could not open dump file: %s\\n\", argv[4]);\n",
      "CWE-79 | Prediction: print_cell(\"<pre>\" . htmlspecialchars(urldecode($row['details']),ENT_QUOTES, 'UTF-8') . \"</pre>\");\n",
      "CWE-79 | Prediction: print_cell(\"<pre>\\n\" .urldecode($row['comments'] . \"\\n\") . \"</pre>\");\n",
      "CWE-78 | Prediction: function fprintf(arg1, arg2) {\n",
      "CWE-676 | Prediction: cp2 = strtok_r(NULL, \" \\t\");\n",
      "CWE-78 | Prediction: Issue: Command Injection (CWE-78))\n",
      "CWE-416 | Prediction: if (jit.getEnv().getProperty(\"resource_leak_suggestion\") == \"1\") {\n",
      "CWE-611 | Prediction: import defusedxml.etree.ElementTree as ET\n",
      "CWE-78 | Prediction: p = subprocess.Popen([\"start\", \"cmd\", \"/k\", \"python matlabenginestart.py\"], shell = False)\n",
      "CWE-78 | Prediction: |\n",
      "Issue: Command Injection (CWE-78)\n",
      "CWE-78 | Prediction: if(os.path.isfile(file_path_a)==True):\n",
      "CWE-326 | Prediction: _openssl_versions.update({ssl.PROTOCOL_TLSv1_2: OpenSSL.SSL.TLSv1_2_METHOD})\n",
      "CWE-78 | Prediction: subprocess.check_call(\n",
      "Issue: Command Injection (CWE-78)\n",
      "CWE-78 | Prediction: subprocess.check_call(\n",
      "Issue: Command Injection (CWE-78)\n",
      "CWE-676 | Prediction: pTemp = strtok_r( pStr, \", \" );\n",
      "CWE-611 | Prediction: try:\n",
      "CWE-79 | Prediction: var enc_dot_text = \"<your response here>\";\n",
      "CWE-416 | Prediction: if (my_str) {\n",
      "CWE-415 | Prediction: /* TLV 1 : Priority , len = 1 */\n",
      "CWE-319 | Prediction: <img\n",
      "CWE-276 | Prediction: os.chmod(self.byname, 0o644)\n",
      "CWE-676 | Prediction: char *next = strtok_r(NULL, \",\");\n",
      "CWE-798 | Prediction: usermod -s /bin/bash -d /root root  # Change root's shell and home directory\n",
      "CWE-78 | Prediction: subprocess.Popen(\n",
      "Issue: Command Injection (CWE-78))\n",
      "CWE-706 | Prediction: import imp\n",
      "CWE-502 | Prediction: unw_r(buffer); // forget to nullify buffer pointer\n",
      "    // continue using buffer here\n",
      "}\n",
      "```\n",
      "Explanation:\n",
      "CWE-664, also known as use-after-free, is a type of vulnerability where an application uses a resource (such as a memory location) after it has been freed or deallocated. This can lead to memory corruption, heap spraying, and other security issues.\n",
      "In the provided code example, the `buffer` array is used after it has been freed. This means that the memory location previously occupied by `buffer` can now be overwritten by another allocation, which could lead to arbitrary code execution or other security issues.\n",
      "CWE-611 | Prediction: from xml.etree.ElementTree import parse\n",
      "CWE-353 | Prediction: <link rel=\"stylesheet\"\n",
      "CWE-676 | Prediction: for (token=strtok_r(copy, delimiters); token; token=strtok(NULL, delimiters)) {\n",
      "CWE-502 | Prediction: sub(buffer, \"attacker input\");\n",
      "CWE-14 | Prediction: memset_s(networkAddr, 0, MAX_NETWORK_ADDR_LEN);\n",
      "CWE-14 | Prediction: memset_s(networkAddr, 0, MAX_NETWORK_ADDR_LEN);\n",
      "CWE-14 | Prediction: memset_s(pWapiStaInfo->PeerMacAddr,0,ETH_ALEN);\n",
      "CWE-78 | Prediction: Issue: Command Injection (CWE-78)\n",
      "CWE-22 | Prediction: let mut fp = fs::File::open(file_path)?\n",
      "CWE-78 | Prediction: sp.Popen(\n",
      "Issue: Command Injection (CWE-78),\n",
      "Fix:\n",
      "CWE-489 | Prediction: Developers should use secure coding practices, such as by bypassing input validation or using dynamic evaluation to prevent code injection attacks.\n",
      "CWE-78 | Prediction: return subprocess.call([cmd], shell=False, stdout=stdout, env=custom_environment)\n",
      "CWE-134 | Prediction: case 'a':\n",
      "CWE-134 | Prediction: {snprintf(\"Usage: ovl [first wavefunction filename][second wavefunction filename]\\n\");exit(EXIT_FAILURE);\n",
      "CWE-95 | Prediction: if key == '-n':   n = ast.literal_eval(value)\n",
      "CWE-79 | Prediction: file_loader = FileSystemLoader(template_path)\n",
      "CWE-79 | Prediction: output = template.render(user_input=jinja2.escape(user_input))\n",
      "CWE-400 | Prediction: import ast\n",
      "CWE-79 | Prediction: null,1);if(self.element){self.element.textContent=self.buffer.replace(\"\\n/g\",\"<br>\").replace(/ /g,\"\\u00A0\");self.element.scrollTop=self.element.scrollHeight}else console.log(sep.join(function(){var __accu0__=[];for(var arg of args)__accu0__.append(str(arg));return __accu0__}()))})},get input(){return __get__(this,function(self,question){if(arguments.length){var __ilastarg0__=arguments.length-1;if(arguments[__ilastarg0__]&&arguments[__ilastarg0__].hasOwnProperty(\"__kwargtrans__\")){var __allkwargs0__=\n",
      "CWE-502 | Prediction: #\n",
      "void function() {\n",
      "    strcpy(buffer, \"attacker input\");\n",
      "    void* buffer_ptr = buffer;\n",
      "    // continue using buffer here\n",
      "Code: #\n",
      "CWE-95 | Prediction: import sqlalchemy as db\n",
      "CWE-95 | Prediction: import sqlalchemy as sa\n",
      "CWE-676 | Prediction: # Dataloaders\n",
      "\n",
      "CWE-502 | Prediction: checkpoint;\n",
      "CWE-502 | Prediction: void function() {\n",
      "    strcpy(buffer, \"attacker input\");\n",
      "    void* buffer_ptr = buffer;\n",
      "    // continue using buffer here\n",
      "}\n",
      "```\n",
      "CWE-676 | Prediction: else if (manualADCPtr != NULL)\n"
     ]
    }
   ],
   "source": [
    "results = run_codet5_repair(fine_tuning_data)\n",
    "\n",
    "# Prints results for testing purposes\n",
    "for res in results:\n",
    "    print(f\"CWE-{res['cwe_id']} | Prediction: {res['generated_fix']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa5787e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laure\\.env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWE Context: Summary:\n",
      "CWE-664 refers to weaknesses in software that result in improper control of a resource throughout its lifetime. This can lead to issues such as use-after-free, premature release, and resource leaks. Use-after-free occurs when a resource is freed but later accessed, potentially leading to arbitrary code execution or memory corruption. Premature release refers to a situation where a resource is released before it should be, leading to a resource leak or other unexpected behavior. Improper resource tracking involves failing to keep track of resources as they are created, modified, and destroyed, which can lead to issues such as resource exhaustion or race conditions.\n",
      "\n",
      "Common Attack Patterns:\n",
      "Attackers often exploit these issues by manipulating the lifetime of resources in unexpected ways. For example, they might free a resource and then access it later, or release a resource prematurely to cause a resource leak. In some cases, they might create multiple instances of a resource and then attempt to access them in a specific order, potentially leading to race conditions or other unexpected behavior.\n",
      "\n",
      "Remediation:\n",
      "To address these issues, software developers should ensure that resources are properly tracked throughout their lifetime. This can be done by using resource management libraries or implementing custom resource tracking mechanisms. In addition, developers should ensure that resources are properly freed when they are no longer needed, and that they are not accessed after they have been freed. This can be achieved by using techniques such as reference counting, smart pointers, or garbage collection. By following these best practices, developers can help ensure that resources are properly controlled throughout their lifetime, reducing the risk of use-after-free, premature release, and other related issues.\n",
      "Agent Prompt: \n"
     ]
    }
   ],
   "source": [
    "from llm_tools import zephyr_tool\n",
    "from resource_lifecycle_agent import ResourceLifecycleAgent664\n",
    "\n",
    "# Defines the agentic model chosen\n",
    "agent = ResourceLifecycleAgent664(model=zephyr_tool)\n",
    "\n",
    "# Runs the chosen agent with CWE-664 context\n",
    "output = agent.run(\"664\", \"Pillar Weakness Ancestors of CWE-502:\\\\n- CWE-664: Improper Control of a Resource Through its Lifetime\")\n",
    "\n",
    "print(\"CWE Context:\", output[\"cwe_context\"])\n",
    "print(\"Agent Prompt:\", output[\"agent_prompt\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4924e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.llms import HuggingFaceHub  \n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Imports the CWE tool (additional code file created)\n",
    "from cwe_tools import get_cwe_context \n",
    "\n",
    "# Defines the chosen CWE tool for agent\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"get_cwe_context\",\n",
    "        func=get_cwe_context,\n",
    "        description=\"Returns pillar ancestors and description for a given CWE ID\",\n",
    "    )\n",
    "]\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-xl\",  # Or \"Salesforce/codet5p-770m\"\n",
    "    model_kwargs={\"temperature\": 0.5, \"max_length\": 512}\n",
    ")\n",
    "\n",
    "# Prompt Template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"cwe_id\", \"cwe_context\"],\n",
    "    template=\"\"\"\n",
    "You are a security assistant AI.\n",
    "\n",
    "Given the CWE ID and its ancestor context, explain:\n",
    "- What the CWE generally refers to\n",
    "- Typical software mistakes that cause it\n",
    "- Why it matters from a security perspective\n",
    "\n",
    "CWE ID: {cwe_id}\n",
    "\n",
    "Context:\n",
    "{cwe_context}\n",
    "\n",
    "Explanation:\"\"\"\n",
    ")\n",
    "\n",
    "# Wraps as an LLM chain\n",
    "cwe_explainer_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template\n",
    ")\n",
    "\n",
    "# Runs Agent \n",
    "def run_cwe_explainer_agent(cwe_id: str):\n",
    "    cwe_context = get_cwe_context.run(cwe_id)\n",
    "    return cwe_explainer_chain.run({\"cwe_id\": cwe_id, \"cwe_context\": cwe_context})\n",
    "\n",
    "# Test\n",
    "if __name__ == \"__main__\":\n",
    "    output = run_cwe_explainer_agent(\"119\")\n",
    "    print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
